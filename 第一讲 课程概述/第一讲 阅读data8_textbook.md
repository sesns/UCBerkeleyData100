# 什么是数据科学

    数据科学是关于通过探索，预测和推论从大量且多样的数据中得出有用的结论

    探索：发现信息中的模式

    预测：使用已知信息去作出有根据的猜测

    推论：包括量化确认度：我们的预测有多精确、我们在数据中发现的模式是否也会出现在新观察中？

    工具

- 对于探索，有描述性统计学、可视化

- 对于预测，有机器学习与优化

- 对于推论，有统计测试和模型

## 运算工具

    本书采用python3及其一些数据可视化工具

## 统计技术

    统计的最重要贡献之一是描述观察和结论之间关系的一致且精确的词汇。

    本文涉及到以下问题：测试假设，估计置信度和预测未知数量等

# 因果和实验

## 概念

实验和结果之间的任何关系被称为**关联**。如果实验导致结果发生，那么这个关联是**因果关系**。

## 实验组和对照组

科学家使用比较来确定实验与结果之间的关联，即实验和结果之间存在联系。他们比较了一组接受实验的个体（**实验组**）的结果，和一组没有接受实验的个体的结果（**对照组**）。

## 混淆

在一项观察研究中，如果实验组和对照组在实验以外的方面有所不同，则很难对因果关系作出结论。 两组之间在**实验以外**的根本区别被称为**混淆因素**，因为当你试图得出结论时，它可能会混淆你。

## 随机化

**避免混淆**的一个很好的方法是，将个体随机分配到实验和对照组，然后将实验给予分配到实验组的人。随机化使两组除了实验之外都相似。如果你能够将个体随机分为实验组和对照组，则你正在进行一项**随机对照试验**（RCT）

有时候，人们在实验中的反应会受到他们知道他们在哪个群体的影响。所以你可能希望进行**盲法实验**，其中个体不知道他们是在实验组还是对照组。为了使它有效，你必须把安慰剂给控制组，这是一种和实验看起来完全一样的东西，但实际上没有效果。

在某些情况下，即使目标是调查因果关系，也不可能进行随机对照实验。

随机化的优点是

- 它使我们能够以数学方式，计算随机化产生实验和对照组的可能性。

- 它使我们能够对实验组和对照组之间的差异作出**精确的数学表述**。这反过来帮助我们对实验是否有效作出正确的结论。

## 因果关系

因果关系的建立往往分两个阶段进行。首先，观察一个关联。接下来，更仔细的分析是否存在因果关系。

## 总结

在本课程中，你将学习如何进行和分析你自己的随机实验。这将涉及比本节更多的细节。

**目前，只需关注主要思想：尝试建立因果关系，如果可能，进行随机对照实验。**

注意：如果你正在进行一项观察研究，你可能能够建立联系而不是因果关系。在根据观察研究得出因果关系的结论之前，要非常小心混淆因素。

    

# Python编程

## 运算符

| 表达式类型 | 运算符     | 示例         | 值       |
|:-----:|:-------:|:----------:|:-------:|
| 加法    | `+`     | `2 + 3`    | 5       |
| 减法    | `-`     | `2 - 3`    | -1      |
| 乘法    | `*`     | `2 * 3`    | 6       |
| 除法    | `/`     | `7 / 3`    | 2.66667 |
| 取余    | `%`     | `7 % 3`    | 1       |
| 指数    | `**`    | `2 ** 0.5` | 1.41421 |
| 括号    | （）[] {} |            |         |

## 数值

整数在 Python 语言中称为`int`值。 它们只能表示没有小数部分的整数（负数，零或正数）

实数在 Python 语言中被称为`float`值（或浮点值）。 他们可以表示全部或部分数字，但有一些限制。

当一个`float`值和一个`int`值，通过算术运算符组合在一起时，结果总是一个`float`值。

在大多数情况下，两个整数的组合形成另一个整数，但任何数字（`int`或`float`）除以另一个将是一个`float`值。

浮点数只能表示任何数字的 15 或 16 位有效数字；剩下的精度就会丢失。 如果一个计算的结果是一个非常大的数字，那么它被表示为无限大。 如果结果是非常小的数字，则表示为零。

## 名称

名称通过赋值语句在 Python 中得到一个值。 在赋值中，名称后面是`=`，再后面是任何表达式。 `=`右边的表达式的值被赋给名称。

名称必须以字母开头，但可以包含字母和数字。 名称不能包含空格；相反，通常使用下划线字符`_`来替换每个空格

## 函数

一些函数默认是可用的，比如`abs`和`round`，但是大部分内置于 Python 语言的函数都存储在一个称为模块的函数集合中。**导入语句用于访问模块**，如`math`或`operator`。

```py
import math
import operator
math.sqrt(operator.add(4, 5))
3.0
```

## 类型

每个值都有一个类型，内建的`type`函数返回任何表达式的结果的类型

## 字符串

单引号和双引号都可以用来创建字符串：`'hi'`和`"hi"`是相同的表达式。 双引号通常是首选，因为它们允许在字符串中包含单引号。

字符串也有一些方法，例如replace、upper

```py
"loud".upper()
'LOUD'
'hitchhiker'.replace('hi', 'ma')
'matchmaker'
s = "train"
t = s.replace('t', 'ing')
u = t.replace('in', 'de')
u
'degrade'
```

## 比较运算符

一个表达式可以包含多个比较

| 比较   | 运算符  | True 示例  | False 示例 |
| ---- | ---- | -------- | -------- |
| 小于   | `<`  | `2 < 3`  | `<`      |
| 大于   | `>`  | `3 > 2`  | `>`      |
| 小于等于 | `<=` | `<=`     | `3 <= 2` |
| 大于等于 | `>=` | `>=`     | `2 >= 3` |
| 等于   | `==` | `==`     | `3 == 2` |
| 不等于  | `!=` | `3 != 2` | `!=`     |

## 集合

Python 中有很多种类的集合，我们在这门课中主要使用数组。

在几个值上调用`make_array`函数，将它们放到一个数组中，这是一种顺序集合。

集合允许我们使用单个名称，将多个值传递给一个函数。 例如，`sum`函数计算集合中所有值的和，`len`函数计算其长度。

## 数组

`make_array`函数可以用来创建数值的数组。

数组也可以包含字符串或其他类型的值，但是**单个数组只能包含单一类型的数据。**

```py
english_parts_of_speech = make_array("noun", "pronoun", "verb", "adverb", "adjective", "conjunction", "preposition", "interjection")
```

数组可以用在算术表达式中来计算其内容。 当数组与单个数组合时，该数与数组的每个元素组合。

```py
(9/5) * highs + 32
array([ 56.48  ,  57.8966,  58.253 ,  59.2952])
```

数组也有方法，这些方法是操作数组值的函数。

```py
highs.size
4
highs.sum()
57.736000000000004
highs.mean()
14.434000000000001
```

## 数组上的函数

`numpy`包，在程序中缩写为`np`，为 Python 程序员提供了创建和操作数组的，方便而强大的函数。

```py
import numpy as np
```

每个这些函数接受数组作为参数，并返回单个值。

| 函数                 | 描述                    |
| ------------------ | --------------------- |
| `np.prod`          | 将所有元素相乘               |
| `np.sum`           | 将所有元素相加               |
| `np.all`           | 测试是否所有元素是真值 （非零数值是真值） |
| `np.any`           | 测试是否任意元素是真值（非零数值是真值）  |
| `np.count_nonzero` | 计算非零元素的数量             |

每个这些函数接受字符串数组作为参数，并返回数组。

| 函数                  | 描述                    |
| ------------------- | --------------------- |
| `np.char.lower`     | 将每个元素变成小写             |
| `np.char.upper`     | 将每个元素变成大写             |
| `np.char.strip`     | 移除每个元素开头或末尾的空格        |
| `np.char.isalpha`   | 每个元素是否只含有字母（没有数字或者符号） |
| `np.char.isnumeric` | 每个元素是否只含有数字（没有字母）     |

每个这些函数接受字符串数组和一个搜索字符串。

| 函数                 | 描述                    |
| ------------------ | --------------------- |
| np.char.count      | 在数组的元素中，计算搜索字符串的出现次数  |
| np.char.find       | 在每个元素中，搜索字符串的首次出现位置   |
| np.char.rfind      | 在每个元素中，搜索字符串的最后一次出现位置 |
| np.char.startswith | 每个字符串是否以搜索字符串起始       |

## 范围

范围是一个数组，按照递增或递减的顺序排列，每个元素按照一定的间隔分开。

范围使用`np.arange`函数来定义，该函数接受一个，两个或三个参数：起始值，终止值和“步长”。

如果将一个参数传递给`np.arange`，那么它将成为终止值，其中`start = 0`，`step = 1`。 两个参数提供了起始值和终止值，`step = 1`。 三个参数明确地提供了起始值，终止值和步长。

**范围始终包含其`start`值，但不包括其`end`值。 它按照`step`计数，并在到达`end`之前停止。**

```py
np.arange(5)
array([0, 1, 2, 3, 4])

np.arange(3, 9)
array([3, 4, 5, 6, 7, 8])

np.arange(3, 30, 5)
array([ 3,  8, 13, 18, 23, 28])


np.arange(1.5, -2, -0.5)
array([ 1.5,  1. ,  0.5,  0. , -0.5, -1. , -1.5])
```

# 表格

## 模块导入

```py
from datascience import *
```

## 创建表格

创建一个新的空表格

```py
Table()
```

`with_columns`方法将新列添加到表中

```py
Table().with_columns('XXX', make_array(8, 34, 5))

Table().with_columns(
    'Number of petals', make_array(8, 34, 5),
    'Name', make_array('lotus', 'sunflower', 'rose'))

flowers = Table().with_columns(
    'Number of petals', make_array(8, 34, 5),
    'Name', make_array('lotus', 'sunflower', 'rose')
)

flowers.with_columns(
    'Color', make_array('pink', 'yellow', 'red')
)
```

通过使用`Table`的`read_table`方法读入CSV文件创建表格

```python
minard = Table.read_table('minard.csv')
minard
```

## 列的选取

`show`展示行，允许我们指定行数，缺省值（没有指定）是表的所有行。

```py
nba_salaries.show(3)
```

通过使用`relabeled`修改列标签。这会创建新的表格，并保留原表格不变。 

```python
minard.relabeled('City', 'City Name')
```

查看表格中列的数量、行的数量

```py
minard.num_columns
5
minard.num_rows
8
```

访问表格中某一列数组，参数为列名或列下标

列下标从0开始

```py
minard.column('Survivors')
array([145000, 140000, 127100, 100000,  55000,  24000,  20000,  12000])

minard.column(4)
array([145000, 140000, 127100, 100000,  55000,  24000,  20000,  12000])
```

访问列的某一条目

```py
minard.column(4).item(0)
145000
minard.column(4).item(5)
24000
```

用选项`PercentFormatter`调用`set_format`方法，使得列中的比例显示为百分比

```py
minard.set_format('Percent Surviving', PercentFormatter)
```

`select`方法创建一个新表，仅仅包含指定的列

```py
minard.select('Longitude', 'Latitude')
minard.select(0, 1)
```

另一种创建新表，包含列集合的方式，是`drop`你不想要的列。

```py
minard.drop('Longitude', 'Latitude', 'Direction')
```

## 行的选取

take()方法指定行，可以传入单个索引，也可以传入一系列索引

```py
nba.take(0)
nba.take(np.arange(3, 6)) //选取第4、5、6行
```

where选取具有指定特征的行，特征由are指定

`where`的第一个参数是列标签，`where`的第二个参数是用于指定特征的方式。

它的输出是一个表格，列与原始表格相同，但只有特征出现的行。

通过重复使用`where`，你可以访问具有多个指定特征的行

```py
original_table_name.where(column_label_string, are.condition)

nba.where('POSITION', 'PG').where('SALARY', are.above(15))
```

这里有一些谓词，你可能会觉得有用。 请注意，`x`和`y`是数字，`STRING`是一个字符串，`Z`是数字或字符串；你必须指定这些，取决于你想要的特征

| 谓词                              | 描述              |
| ------------------------------- | --------------- |
| `are.equal_to(Z)`               | 等于`Z`           |
| `are.above(x)`                  | 大于`x`           |
| `are.above_or_equal_to(x)`      | 大于等于`x`         |
| `are.below(x)`                  | 小于`x`           |
| `are.below_or_equal_to(x)`      | 小于等于`x`         |
| `are.between(x, y)`             | 大于等于`x`，小于`y`   |
| `are.strictly_between(x, y)`    | 大于`x`，小于`y`     |
| `are.between_or_equal_to(x, y)` | 大于等于`x`，小于等于`y` |
| `are.containing(S)`             | 包含字符串`S`        |

| 谓词                    | 描述     |
| --------------------- | ------ |
| `are.not_equal_to(Z)` | 不等于`Z` |
| `are.not_above(x)`    | 不大于`x` |

## 对行排序

`sort`的参数是列标签或索引，默认升序

参数descending=True表示降序

```py
nba_salaries.sort('PLAYER')
nba.sort('SALARY', descending=True)
```

# 可视化

定量：拥有数值的变量，也称数值变量

## 散点图

散点图展示两个数值变量之间的关系

`Table`的`scatter`方法绘制一个散点图，由表格的每一行组成。它的第一个参数是要在横轴上绘制的列标签，第二个参数是纵轴上的列标签。

```py
actors.scatter('Number of Movies', 'Total Gross')


no_outlier = actors.where('Number of Movies', are.above(10))
no_outlier.scatter('Number of Movies', 'Average per Movie')
```

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-10-27.png)

## 线形图

线形图是最常见的可视化图形之一，通常用于研究时序型的趋势和模式。

`Table`的`plot`方法产生线形图。 它的两个参数与散点图相同：首先是横轴上的列，然后是纵轴上的列

```python
century_21.plot('Year', 'Number of Movies')
```

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-14-05.png)

## 分布表

 分布表，列包括分布变量、每个变量的频率

| Flavor     | Number of Cartons |
| ---------- | ----------------- |
| Chocolate  | 16                |
| Strawberry | 5                 |
| Vanilla    | 9                 |

分类变量“口味”的值是巧克力，草莓和香草。 表格显示了每种口味的纸盒数量。

如何生成分布表？`Table`的`group`方法为我们计算分布变量不同的值出现在表中的频率，生成分布表

```py
movies_and_studios.group('Studio')
```

## 条形图

分布表，列包括分布变量、每个变量的频率

| Flavor     | Number of Cartons |
| ---------- | ----------------- |
| Chocolate  | 16                |
| Strawberry | 5                 |
| Vanilla    | 9                 |

分类变量“口味”的值是巧克力，草莓和香草。 表格显示了每种口味的纸盒数量。

条形图是可视化类别分布的熟悉方式。 它为每个类别显示一个条形。 条形的间隔相等，宽度相同。 每个条形的长度与相应类别的频率成正比。

我们使用横条绘制条形图，因为这样更容易标注条形图。 所以`Table`的方法称为`barh`。 它有两个参数：第一个是类别的列标签，第二个是频率的列标签。

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-17-37.png)

除了纯粹的视觉差异之外，条形图和我们在前面章节中看到的两个图表之间还有一个重要的区别。 它们是散点图和线图，两者都显示两个数值变量 - 两个轴上的变量都是数值型的。 相比之下，条形图的一个轴上是类别，在另一个轴上具有数值型频率，这对图表有影响。

## 频率分布直方图

在直角坐标系中，用横轴表示随机变量的取值，横轴上的每个小区间对应一个组的组距，作为小矩形的底边；纵轴表示频率(组频数/总频数=频率)，并用它作小矩形的高，以这种小矩形构成的一组图称为频率直方图。

在直角坐标系中，用横轴表示随机变量的取值，横轴上的每个小区间对应一个组的组距，作为小矩形的底边；纵轴表示频率密度(频率/组距)，并用它作小矩形的高，以这种小矩形构成的一组图称为频率分布直方图（hist）。

在频率分布直方图中，每个小矩形的面积等于相应桶中数值数量占总数量的百分比。所有小矩形的面积“总计为 1”。

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-44-57.png)

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-45-17.png)

hist方法生成列中值的直方图。 可选的单位参数用于两个轴上的标签。 直方图显示数值的分布

将横轴划分成了若干个区间，每个区间称为“bin”，也称为桶。`bin`包含左端点的数据，但不包含右端点的数据。我们使用符号`[a, b)`表示从`a`开始并在`b`结束但不包括`b`的桶。

```py
millions.hist('Adjusted Gross', unit="Million Dollars")
```

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-27-44.png)

可选参数`bins`可以与`hist`一起使用来指定桶的端点。它必须由一系列数字组成，这些数字以第一个桶的左端开始，以最后一个桶的右端结束。我们首先将桶中的数字设置为`300,400,500`等等，以`2000`结尾。

```py
millions.hist('Adjusted Gross', bins=np.arange(300,2001,100), unit="Million Dollars")
```

可以使用`bin`方法从一个表格中计算出桶中的值的数量，该方法接受列标签或索引，以及可选的序列或桶的数量。

```py
bin_counts = millions.bin('Adjusted Gross', bins=np.arange(300,2001,100))
```

| bin  | Adjusted Gross count |
| ---- | -------------------- |
| 300  | 81                   |
| 400  | 52                   |
| 500  | 28                   |
| 600  | 16                   |
| 700  | 7                    |
| 800  | 5                    |
| 900  | 3                    |
| 1000 | 1                    |
| 1100 | 3                    |
| 1200 | 2                    |
| 1300 | 0                    |
| 1400 | 0                    |
| 1500 | 1                    |
| 1600 | 0                    |
| 1700 | 1                    |
| 1800 | 0                    |
| 1900 | 0                    |
| 2000 | 0                    |

## **直方图使用频率密度作为纵轴的好处**

当桶长度不等时，采用频率作为纵轴的直方图看过去很容易造成误导，无法突出显示数值的密集程度，而使用频率密度作为纵轴的直方图可以突出显示数值的密集程度

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-50-33.png)

上图是采用频数作为纵轴的直方图（但是总频数相等，因此其实 这个图和采用频率作为纵轴的直方图差不多）

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_21-54-52.png)

上图是采用频率密度作为纵轴的直方图。虽然范围`[300,400)`和`[400,600)`具有几乎相同的计数，但前者的高度是后者的两倍，因为它只有一半的宽度。 `[300,400)`中的值的密度是`[400,600)`中的密度的两倍。

**频率分布直方图（hist）帮助我们可视化数轴上数据最集中的地方，特别是当桶不均匀的时候。**

## 条形图和直方图的区别

- 条形图为每个类别展示一个数量。 它们通常用于显示**类别变量**的分布。 直方图显示**数值变量**的分布。
- 条形图中的所有条形都具有相同的宽度，相邻的条形之间有相等的间距。 直方图的条形可以具有**不同的宽度**，并且是连续的。
- 条形图中条形的**长度**（或高度，如果垂直绘制）与每个类别的值成正比。 直方图中条形的高度是密度的度量；直方图中的条形的**面积**与桶中的条目数量成正比。

## 重叠的图表

在这一章中，我们学习了如何通过绘制图表来显示数据。 这种可视化的常见用法是比较两个数据集。 在本节中，我们将看到如何叠加绘图，即将它们绘制在单个图形中，拥有同一对坐标轴

为了使重叠有意义，重叠的图必须表示相同的变量并以相同的单位进行测量。

**为了绘制重叠图，可以用相同的方法调用`scatter`，`plot`和`barh`方法。 对于`scatter`和`plot`，一列必须作为所有叠加图的公共横轴。 对于`barh`，一列必须作为一组类别的公共轴。 一般的调用看起来像这样：**

```py
name_of_table.method(column_label_of_common_axis, array_of_labels_of_variables_to_plot)
```

更常见的是，我们首先仅仅选取图表所需的列。之后通过指定共同轴上的变量来调用方法。

```py
name_of_table.method(column_label_of_common_axis)
```

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_22-09-41.png)
![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_22-09-47.png)
![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-19_22-10-21.png)

# 函数和表格

## 定义函数

我们通过编写`def`来开始定义任何函数。

一个函数可以有多个参数

 文档字符串通常在开始和结束处使用三个引号来定义，这允许字符串跨越多行。 第一行通常是函数的完整但简短的描述，而下面的行则为将来的用户提供了进一步的指导。

函数通过将参数表达式放入函数名称后面的括号来调用。 任何独立定义的函数都是这样调用的。 你也看到了方法的例子，这些方法就像函数一样，但是用点符号来调用

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/d3dbf73cbcc6c06d81d679245441855c_566x429.jpg)

```py
# Our first function definition

def double(x):
    """ Double x """
    return 2*x
```

```py
any_name = 42
double(any_name)
84


double(make_array(3, 4, 5))
array([ 6,  8, 10])
```

## 在列上应用函数

`apply`方法在列的每个元素上调用一个函数，返回函数调用完后的新数组。输入值的列的名称必须是字符串，仍然出现在引号内

```py
def cut_off_at_100(x):
    """The smaller of x and 100"""
    return min(x, 100)

//ages是一张表，’Age‘是列名
ages.apply(cut_off_at_100, 'Age')
array([ 17, 100,  52, 100,   6, 100])
```

## 引用函数

我们可以引用任何函数，通过写下它的名字，而没有实际调用它

```py
cut_off_at_100
<function __main__.cut_off_at_100>
```

我们可以为函数定义新名称。

```py
cut_off = cut_off_at_100
```

## 变量分类下聚合函数的使用

    使用group按分类变量对表格进行分组，统计每组内数值个数

    第一个参数为列标签，若干列作为分类变量

    第二个可选参数为函数，用于聚合组内的值，就像sql里的聚合函数

- sum

- max

- ……

```py
cones.group('Flavor', max)
cones.group('Flavor', sum)
more_cones.group(['Flavor', 'Color'])
more_cones.group(['Flavor', 'Color'], sum)
```

| Flavor     | Color       | Price sum |
| ---------- | ----------- | --------- |
| bubblegum  | pink        | 4.75      |
| chocolate  | dark brown  | 10.5      |
| chocolate  | light brown | 4.75      |
| strawberry | pink        | 8.8       |

| Flavor     | count |
| ---------- | ----- |
| bubblegum  | 1     |
| chocolate  | 3     |
| strawberry | 2     |

## 数据透视表

两个分类变量，列是分类变量的值，行是另一分类变量的值，形成了一个网格，由两个分类变量定位一个位置，该位置的值表示该组下的行个数

采用pivot方法生成数据透视表，`pivot`的第一个参数是列标签，包含的值将用于在结果中形成新的列。第二个参数是用于行的列标签。结果提供了原始表的所有行的计数

像`group`一样，`pivot`可以和其他参数一同使用，来发现每个类别组合的特征。名为`values`的第三个可选参数表示一列值，它们替换网格的每个单元格中的计数。所有这些值将不会显示，但是；第四个参数`collect`表示如何将它们全部汇总到一个聚合值中，来显示在单元格中。

由`pivot`生成的表格更易于阅读，因而更易于分析。 透视表的优点是它将分组的值放到相邻的列中，以便它们可以进行组合和比较。

```py
more_cones.pivot('Flavor', 'Color')
```

| Color       | bubblegum | chocolate | strawberry |
| ----------- | --------- | --------- | ---------- |
| dark brown  | 0         | 2         | 0          |
| light brown | 0         | 1         | 0          |
| pink        | 1         | 0         | 2          |

```py
more_cones.pivot('Flavor', 'Color', values='Price', collect=sum)
```

| Color       | bubblegum | chocolate | strawberry |
| ----------- | --------- | --------- | ---------- |
| dark brown  | 0         | 10.5      | 0          |
| light brown | 0         | 4.75      | 0          |
| pink        | 4.75      | 0         | 8.8        |

| Personal Income     | Bachelor's degree or higher | College, less than 4-yr degree | High school or equivalent | No high school diploma |
| ------------------- | --------------------------- | ------------------------------ | ------------------------- | ---------------------- |
| A: 0 to 4,999       | 6.75                        | 12.67                          | 18.46                     | 28.29                  |
| B: 5,000 to 9,999   | 3.82                        | 10.43                          | 9.95                      | 14.02                  |
| C: 10,000 to 14,999 | 5.31                        | 10.27                          | 11                        | 15.61                  |
| D: 15,000 to 24,999 | 9.07                        | 17.3                           | 19.9                      | 20.56                  |
| E: 25,000 to 34,999 | 8.14                        | 14.04                          | 14.76                     | 10.91                  |
| F: 35,000 to 49,999 | 13.17                       | 14.31                          | 12.44                     | 6.12                   |
| G: 50,000 to 74,999 | 18.7                        | 11.37                          | 8.35                      | 3.11                   |
| H: 75,000 and over  | 35.03                       | 9.62                           | 5.13                      | 1.38                   |

## 按列连接表

join方法连接两个表

```py
table1.join(table1_column_for_joining, table2, table2_column_for_joining)
```

| Flavor     | Price |
| ---------- | ----- |
| strawberry | 3.55  |
| vanilla    | 4.75  |
| chocolate  | 6.55  |
| strawberry | 5.25  |
| chocolate  | 5.75  |

| Kind       | Stars |
| ---------- | ----- |
| strawberry | 2.5   |
| chocolate  | 3.5   |
| vanilla    | 4     |

```py
rated = 表1.join('Flavor', 表2, 'Kind')
rated
```

| Flavor     | Price | Stars |
| ---------- | ----- | ----- |
| chocolate  | 6.55  | 3.5   |
| chocolate  | 5.75  | 3.5   |
| strawberry | 3.55  | 2.5   |
| strawberry | 5.25  | 2.5   |
| vanilla    | 4.75  | 4     |

**注意**，由于`join`中的第二个表用于扩充第一个表，所以重要的是，第一个表中的每一行在第二个表中只有一个匹配的行。如果第一个表中的某一行在第二个表中没有匹配项，则信息可能丢失。如果第一个表中的某一行在第二个表中有多个匹配项，那么`join`将只选择表2中匹配的第一行，这也是一种信息丢失

## 绘制地图

我们可以使用`Marker.map_table`来绘制地图。 该函数在一个表格上进行操作，该表格的列依次是纬度，经度以及每个点的可选标识符。

| station_id | name                              | lat     | long     | dockcount | landmark | installation |
| ---------- | --------------------------------- | ------- | -------- | --------- | -------- | ------------ |
| 2          | San Jose Diridon Caltrain Station | 37.3297 | -121.902 | 27        | San Jose | 8/6/2013     |
| 3          | San Jose Civic Center             | 37.3307 | -121.889 | 15        | San Jose | 8/5/2013     |
| 4          | Santa Clara at Almaden            | 37.334  | -121.895 | 11        | San Jose | 8/6/2013     |
| 5          | Adobe on Almaden                  | 37.3314 | -121.893 | 19        | San Jose | 8/5/2013     |
| 6          | San Pedro Square                  | 37.3367 | -121.894 | 15        | San Jose | 8/7/2013     |
| 7          | Paseo de San Antonio              | 37.3338 | -121.887 | 15        | San Jose | 8/7/2013     |
| 8          | San Salvador at 1st               | 37.3302 | -121.886 | 15        | San Jose | 8/5/2013     |
| 9          | Japantown                         | 37.3487 | -121.895 | 15        | San Jose | 8/5/2013     |
| 10         | San Jose City Hall                | 37.3374 | -121.887 | 15        | San Jose | 8/6/2013     |
| 11         | MLK Library                       | 37.3359 | -121.886 | 19        | San Jose | 8/6/2013     |

```py
Marker.map_table(stations.select('lat', 'long', 'name'))
```

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/Snipaste_2023-01-20_21-52-38.png)

# 随机性

## python用于随机选择的模块

 在`numpy`中有一个叫做`random`的子模块，它包含许多涉及随机选择的函数。 其中一个函数称为`choice`。 它从一个数组中随机选取一个项目，选择任何项目都是等可能的。 函数调用是`np.random.choice(array_name)`，其中`array_name`是要从中进行选择的数组的名称。

```py
two_groups = make_array('treatment', 'control')
np.random.choice(two_groups)
'treatment'
```

可以通过提供第二个参数来重复np.random.choice这个过程，它是重复这个过程的次数。

```py
np.random.choice(two_groups, 10)
array(['treatment', 'control', 'treatment', 'control', 'control',
       'treatment', 'treatment', 'control', 'control', 'control'], 
      dtype='<U9')
```

## 比较数组和值

如果我们比较一个数组和一个值，则数组的每个元素都与该值进行比较，并将比较结果求值为布尔值数组。

```py
tosses = make_array('Tails', 'Heads', 'Tails', 'Heads', 'Heads')
tosses == 'Heads'
array([False,  True, False,  True,  True], dtype=bool)
```

`numpy`方法`count_nonzero`计算数组的非零（即`True`）元素的数量。

```py
np.count_nonzero(tosses == 'Heads')
3
```

## 条件语句

```py
if <if expression>:
    <if body>
elif <elif expression 0>:
    <elif body 0>
elif <elif expression 1>:
    <elif body 1>
...
else:
    <else body>
```

## for循环

```py
for i in np.arange(3):
    print(i)
0
1
2
```

### 扩展数组

调用`np.append(array_name，value)`将求出一个新的数组，它是由`value`扩展的`array_name`，原数组不变

```py
pets = make_array('Cat', 'Dog')
np.append(pets, 'Another Pet')
array(['Cat', 'Dog', 'Another Pet'], 
      dtype='<U11')


pets
array(['Cat', 'Dog'], 
      dtype='<U3')
```

可以通过将扩展后的数组赋给原始数组来达到拓展原数组的效果

```py
pets = np.append(pets, 'Another Pet')
pets
array(['Cat', 'Dog', 'Another Pet'], 
      dtype='<U11')
```

## 事件不会发生的时候

P(事件不发生)=1-P（事件发生）

## 所有结果等可能的时候

P（一个事件发生）= 该事件发生的结果数/所有结果数

## 两个事件必须同时发生时

通常，我们拥有乘法规则：

两个事件同时发生的概率，等于第一个事件发生的概率，乘上第一个事件发生的情况下第二个事件发生的概率。

## 事件以两种不同的方式发生

通常，我们拥有加法规则：

事件发生的概率，等于以第一种方式发生的概率，加上以第二种方式发生的概率。

只要事件正好以两种方式之一发生。

### 至少有……的概率

P(至少有……)=1-P(全都不)

## 总体

总体为研究对象（可以是物体的某个属性，或某种数量指标）的全体

当总体为数值指标时，总体是一个随机变量，表达的是所有的取值

例如，想调查某校的学生身高，身高这个数量指标就是研究对象。全校学生的身高值是随机变量，不妨设为X，变量对应有取值范围，这里的范围就是全校学生的身高取值。X就是总体，也称之为总体X。

## 确定性样本

当你只是简单地指定，你要选择的集合中的哪些元素时，就不会涉及任何几率，称为确定性样本

## 概率样本

以概率进行抽样得到的样本

## 系统样本

想象一下，总体的所有元素都列出在序列中。 抽样的一种方法是，先从列表中选择一个随机的位置，然后是它后面的等间隔的位置。样本由这些位置上的元素组成。这样的样本被称为系统样本。

# 经验分布

## 概率分布

值在理论上的概率分布

## 经验分布

通过观测数据得到的值的分布 

## 抽样API

`Table`的`sample`方法，它带放回地随机抽取表中的行。它的参数是样本量，它返回一个由选定的行组成的表。 `with_replacement=False`的可选参数指定了应该抽取样本而不放回。

## 平均定律

如果偶然的实验在相同的条件下独立重复（无论所有其他重复的结果如何，每个重复都以相同的方式执行。），那么从长远来看，事件发生的频率越来越接近事件的理论概率。

大型随机样本的经验分布类似于总体的分布，经验直方图类似于总体的直方图

大型随机样本的统计量的经验直方图类似于统计量的概率直方图。

> 如果我们能够生成所有可能的大小为 1000 （例子）的随机样本，我们就可以知道所有可能的统计量（样本中位数），以及所有这些值的概率，从而 得到统计量的概率直方图
> 
> 但在许多情况下（包括这个），所有可能的样本数量足以超过计算机的容量，概率的纯粹数学计算可能有些困难
> 
> 我们知道，如果样本量很大，并且如果重复抽样过程无数次，那么根据平均定律，统计量的经验直方图（通过抽样得到的统计量）可能类似于统计量的概率直方图。

## 参数

与总体相关的数量被称为参数

## 统计量

统计量是使用样本中数据计算的任何数字，例如样本中位数

## 模拟统计量

我们将使用以下步骤来模拟样本中位数。 你可以用任何其他样本量来替换 1000 的样本量，并将样本中位数替换为其他统计量。

第一步：生成一个统计量。 抽取大小为 1000 的随机样本，并计算样本的中位数。 注意中位数的值。

第二步： 重复步骤 1 多次，每次重新抽样，生成更多的统计值。

第三步：结果可视化。 在第二步结束时，你将会记录许多样本中位数，每个中位数来自不同的样本。 你可以在表格中显示所有的中位数。 你也可以使用直方图来显示它们 - 这是统计量的经验直方图。

# 假设检验

## 从分布中抽样

从一个分布中抽样：一组XXX以及它们的比例。

为此，我们使用函数`proportions_from_distribution`。 它有三个参数：

- 表名
- 包含比例的列的标签
- 样本大小

该函数执行带放回地随机抽样，并返回一个新的表，该表多出了一列`Random Sample`，是随机样本中所出现的比例。

## 两个分布的距离（总变异距离）

函数`total_variation_distance`返回两个数组中的分布的 TVD。

```py
def total_variation_distance(distribution_1, distribution_2):
    return np.abs(distribution_1 - distribution_2).sum()/2
```

函数`table_tvd`使用函数`total_variation_distance`来返回表的两列中的分布的 TVD。

```py
def table_tvd(table, label, other):
    return total_variation_distance(table.column(label), table.column(other))
```

例子：jury表

| Ethnicity | Eligible | Panels | Difference |
| --------- | -------- | ------ | ---------- |
| Asian     | 0.15     | 0.26   | 0.11       |
| Black     | 0.18     | 0.08   | -0.1       |
| Latino    | 0.12     | 0.08   | -0.04      |
| White     | 0.54     | 0.54   | 0          |
| Other     | 0.01     | 0.04   | 0.03       |

```py
table_tvd(jury, 'Eligible', 'Panels')
0.14000000000000001
```

## 假设校验

    详见 https://www.zhihu.com/question/23149768

#### 一.假设

原假设

备选假设（与原假设相反）

#### 二.原假设下的校验统计量

**为了在这两个假设之间作出决策，我们必须选择一个统计量作为我们决策的依据。 这被称为检验统计量。**

在原假设成立下，通过大量随机抽样得到校验统计量的经验分布（通过经验分布来近似检验统计量的概率分布）

根据所观测数据的分布得到校验统计量的观测值

#### 三.校验的结论

比较 校验统计量的经验分布及观测值，看观测到的数据支持原假设还是备选假设，如果数据不支持原假设，我们说检验拒绝了原假设。

如何比较？根据P值

- 如果 P 值小于 5%，结果称为“统计学显着”。

- 如果 P 值更小 - 小于 1%，结果被称为“高度统计学显着”。

- 在这两种情况下，检验的结论是数据支持备选假设。

P值

- P值就是当原假设为真时，得到当前样本观察结果或者更极端（概率更小 ，即朝着备选假设方向）的结果出现的概率。

- 如果P值很小，说明原假设情况的发生的概率很小，而如果出现了，根据[小概率原理](https://baike.baidu.com/item/%E5%B0%8F%E6%A6%82%E7%8E%87%E5%8E%9F%E7%90%86/10876969?fromModule=lemma_inlink)，我们就有理由拒绝原假设，P值越小，我们拒绝原假设的理由越充分。

- 总之，P值越小，表明结果越显著。但是检验的结果究竟是“显著的”、“中度显著的”还是“高度显著的”需要我们自己根据P值的大小和实际问题来解决。

## 做出错误结论概率

该概率==P值的截断值（例如常用的5%）

为什么会做出错误结论，因为所观测到的数据属于极端情况 的这件事是可能出现的，虽然说概率很小

# 估计

## 百分位数

令`p`为 0 到 100 之间的数字。集合的第`p`个百分位数是集合中的（一定条件）的最小值，它>=`p%`的所有值。

通过这个定义，可以计算任何值的集合的任何 0 到 100 之间的百分位数，并且它始终是集合的一个元素。

实际上，假设集合中有`n`个元素。 要找到第`p`个百分位数：

- 对集合升序排序。
- 计算`n`的`p%`：`(p/100) * n`。叫做`k`。
- 如果`k`是一个整数，则取有序集合的第`k`个元素。
- 如果`k`不是一个整数，则将其四舍五入到下一个整数，并采用有序集合的那个元素。

## 四分位数

数值集合的第一个四分位数是第 25 个百分分数。 这个术语（quartile）来自第一个季度（quarter）。 第二个四分位数是中位数，第三个四分位数是第 75 个百分位数。

## percentile函数

`percentile`函数接受两个参数：一个 0 到 100 之间的等级，和一个数组。它返回数组相应的百分位数。

## 自举法

 由于从总体中生成新样本是不可行的，自举法通过称为重采样的方法生成新的随机样本：新样本从原始样本中随机抽取。

**以下是自举法的步骤，用于生成类似总体的另一个随机样本：**

- 将原始样本看做总体。
- 从样本中随机抽取样本，与原始样本**大小相同**。

 按照平均定律，原始样本的分布可能与总体相似，所有“二次样本”的分布可能与原始样本相似。 因此，所有二次样本的分布也可能与总体相似。

使用`sample`方法而没有指定样本大小时，默认情况下样本大小等于用于抽取样本的表的行数。 这是完美的自举！

我们重复自举过程，得到自举样本统计量的经验分布，自举样本的数量将被称为重复数量。

```py
resample_1 = our_sample.sample()
```

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed/7dasfasga%20-%20%E5%89%AF%E6%9C%AC%20-%20%E5%89%AF%E6%9C%AC%20-%20%E5%89%AF%E6%9C%AC.png)

## 估计量是否捕获了总体的未知参数/置信区间

我们要估计的总体未知参数正好落在二次样本的统计量的经验直方图中间，而不是尾部的几率有多少？ 

要回答这个问题，我们必须定义“中间”。 让我们将它看做“总体未知参数落在二次样本的统计量的中间 95%”。

有个例子，以下过程来估计总体中位数：

- 从总体中随机抽取一个大样本。  

- 自举你的随机样本，并从新的随机样本中获取估计量。  

- 重复上述步骤数千次，并获得数千个估计量。  

- 挑选所有估计量的“中间 95％”的区间。  

- 这给了你一个估计量的区间。现在，如果重复整个过程 100 次，会得到 100 个区间，那么 100 个区间中的大约 95 个将包含总体的参数。

换句话说，

- 从总体中随机抽取一个大样本。

- 自举你的随机样本，并从新的随机样本中获取估计量。

- 重复上述步骤数千次，并获得数千个估计量。

- 挑选所有估计量的“中间 95％”的区间。

**通过以上方法得到的区间，有95%的概率包含要估计的总体未知参数，即95%置信区间。当然不仅仅是95%，取任意水平都行**

## 自举法的注意事项

自举法应该以大型随机样本开始。如果你不这样做，该方法可能无法正常工作。它的成功基于大型随机样本（因此也从样本中重采样）

为了近似统计量的概率分布，最好多次复制重采样过程。在我们的例子中，我们使用了 5000 次重复，但一般会推荐 10000 次。

自举百分位数方法适用于基于大型随机样本，估计总体中位数或均值。

但是，它也有其局限性，所有的估计方法也是如此。例如，在以下情况下，它预期没有效果。

- 目标是估计总体中的最小值或最大值，或非常低或非常高的百分位数，或受总体中稀有元素影响较大的参数。
- 统计量的概率分布不是近似钟形的。
- 原始样本非常小，比如 10 或 15。

## 使用置信区间来检验假设

正态分布下，可以使用置信区间来检验假设，如果你正在测试总体平均值是否是特定值 x，并且你使用的 5% 截断值作为 P 值，那么**如果 x 不在平均值的 95% 置信区间内**，你将拒绝原零假设。

如果你使用 1% 的截断值作为 P 值，你必须检查，原假设中指定的值是否在总体均值的 99% 置信区间内。

# 为什么均值很重要

## 均值API

`np.average`和`np.mean`方法返回数组的均值。

```py
not_symmetric = make_array(2, 3, 3, 9)
np.average(not_symmetric)
4.25
np.mean(not_symmetric)
4.25
```

### 基本性质

上面的定义和例子指出了均值的一些性质。

- 它不一定是集合中的一个元素。
- 即使集合的所有元素都是整数，也不一定是整数。
- 它在集合的最小值和最大值之间。
- 它不一定在两个极值的正中间；集合中一半的元素并不总是大于均值。
- 如果集合含有一个变量的值，以指定单位测量，则均值也具有相同的单位。

### 均值的性质

如果一个集合只包含 1 和 0，那么集合的总和就是集合中 1 的数量，集合的均值就是 1 的比例。

```py
zero_one = make_array(1, 1, 1, 0)
sum(zero_one)
3
np.mean(zero_one)
0.75
```

## 均值和直方图

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed@master/Snipaste_2023-01-29_16-34-12.png)

集合的平均值仅取决于不同的值及其比例， 换句话说，集合的平均值仅取决于集合中不同的值及其分布。因此，如果两个集合具有相同的分布并且不同的值也相同，则它们具有相同的均值。

例如，这里是另一个集合，它的分布与`not_symmetric`相同，因此均值也相同。

```py
not_symmetric
array([2, 3, 3, 9])
same_distribution = make_array(2, 2, 3, 3, 3, 3, 9, 9)
np.mean(same_distribution)
4.25
```

## 均值与中位数差别的一个例子

`sf2015`表包含 2015 年旧金山城市员工的薪水和福利数据。与以前一样，我们将我们的分析仅限于那些等价于至少就业半年的人。

```py
sf2015 = Table.read_table('san_francisco_2015.csv').where('Salaries', are.above(10000))
```

我们前面看到了，最高薪资高于 60 万美元，但绝大多数雇员的薪资低于 30 万美元。

```py
sf2015.select('Total Compensation').hist(bins = np.arange(10000, 700000, 25000))
```

![](https://box.kancloud.cn/f41d4ffc67e30481c517502e0c946c5e_473x337.png)

这个直方图向右偏斜；它的右侧有个尾巴。

平均值拉向了尾巴的方向。 所以我们预计平均薪酬会比中位数大，事实确实如此。

```py
compensation = sf2015.column('Total Compensation')
percentile(50, compensation)
110305.78999999999
np.mean(compensation)
114725.98411824222
```

大量总体的收入分布往往是右偏的。 当总体的大部分收入中到低，但很小一部分收入很高时，直方图的右侧有条细长的尾巴。

**平均收入受这条尾巴的影响：尾巴向右延伸得越远，平均值就越大。 但中位数不受分布极值的影响。 这就是经济学家经常用收入分布的中位数来代替平均值的原因。**

## 方差

![](https://cdn.jsdelivr.net/gh/sesns/picgo_bed@master/Snipaste_2023-01-29_17-12-52.png)

方差和原始变量不是一个量纲，因为它的单位是原始变量的平方。 这使得解释非常困难。

## 标准差

标准差（SD）：方差（偏差平方的均值）的算术平方，它大致衡量集合中的数字与其平均水平的差距。

可以使用函数`np.std`来计算数组中值的标准差：

```py
np.std(any_numbers)
3.6314597615834874
```

## 如何度量数据集中一个数与均值的距离

我们采用标准差作为数据集中一个数与均值的距离的单位

### 切比雪夫边界

我们注意到在所有的数值数据集中，大部分条目都在“均值上下几个标准差”的范围内。，

俄罗斯数学家切比雪夫（Pafnuty Chebychev，1821-1894）证明了这个结论，使我们的粗略陈述更加精确。

- 对于数据集中所有数据和数字`z`，“数据集均值上下`z`个标准差”范围内的数据比例至少为1-1/(z^2)

具体来说，对于每个列表：

在“均值上下两个标准差”范围内的比例至少是`1 - 1/4 = 0.75`

在“均值上下三个标准差”范围内的比例至少为`1 - 1/9 ≈ 0.89`

在“均值上下 4.5 个标准差”范围内的比例至少为`1 - 1/4.5^2 ≈ 0.95`

## 标准单位

我们采用标准差作为数据集中一个数与均值的距离的单位，

如何将一个值转换为标准单位？

- （该值-均值）/标准差

因此我们就可以把直方图的横轴按标准单位来表示

![](https://box.kancloud.cn/c69df1d066f9b81faba97554bbcef878_452x298.png)

## 标准正态曲线

标准正态曲线的方程令人印象深刻。 但是现在，最好把它看作是变量直方图的平滑轮廓，变量以标准单位测量并具有钟形分布。

![](https://box.kancloud.cn/b45283b439142a6e5b4e0d9e4b6e74c4_447x313.png)

在标准正态曲线的横轴上，这些值是标准单位。

在标准正态曲线的竖轴上，这些值是频率密度。

曲线下面的总面积是1

曲线是对称的。所以如果一个变量具有这个分布，它的平均值和中位数都是0，标准差是1。

曲线的拐点在 -1 和 +1 处。

由于我们将曲线视为平滑的直方图，因此我们希望用曲线下方的面积来表示数据总量的比例。

平滑曲线下的面积通常是通过微积分来计算的，使用一种称为积分的方法。然而，一个数学的事实是，标准的正态曲线不能通过任何微积分方式来积分。

因此，曲线下方的面积必须近似。这就是几乎所有的统计教科书，都带有曲线下方的面积的原因。这也是所有统计系统，包括 Python 模块在内，都包含提供这些面积的优秀近似的方法的原因。

```py
from scipy import stats
```

## 标准正态的累积分布函数（CDF）

用于求出正态曲线下的面积的基本函数是`stats.norm.cdf`。 它接受一个数值参数，并返回曲线下，该数值的左侧的所有面积。 它在形式上被称为标准正态曲线的“累积分布函数”。 在口语里缩写为 CDF。

让我们使用这个函数来求出标准正态曲线下，`z=1`左侧的面积。

![](https://box.kancloud.cn/74bf461cddc3abc827932dd7ec3adbd1_447x313.png)

阴影区域的数值可以通过调用`stats.norm.cdf`来求出。

```py
stats.norm.cdf(1)
0.84134474606854293
```

这大概是 84%。 现在我们可以使用曲线的对称性，以及曲线下面的总面积为 1 事实，来求出其他面积。

`z = 1`右侧的面积大概是`100% - 84% = 16%`。

![](https://box.kancloud.cn/e3b57d88b4fbf7f607318df48e5afbec_447x313.png)

```py
1 - stats.norm.cdf(1)
0.15865525393145707
```

`z = -1`和`z = 1`之间的面积可以用几种不同的方式来计算。 它是下面的曲线下方的金色区域。

![](https://box.kancloud.cn/b98c8cceadaaed82e0c167221ba85016_447x313.png)

例如，我们可以将面积计算为“`100% -`两个相等的尾巴”，结果大致是`100% - 2X16% = 68%`。

或者我们可以注意到，`z = 1`和`z = -1`之间的区域等于`z = 1`左边的所有区域，减去`z = -1`左边的所有区域。

```py
stats.norm.cdf(1) - stats.norm.cdf(-1)
0.68268949213708585
```

通过类似的计算，我们看到`-2`和`2`之间的区域大约是 95%。

![](https://box.kancloud.cn/8367bc7f055d8957a3c5a015f2c527b0_447x313.png)

```py
stats.norm.cdf(2) - stats.norm.cdf(-2)
0.95449973610364158
```

## 均值若干个标准差范围内数据的比例

| Percent in Range | All Distributions: Bound | Normal Distribution: Approximation |
| ---------------- | ------------------------ | ---------------------------------- |
| 均值上下一个标准差        | 至少 0%                    | 约 68%                              |
| 均值上下两个标准差        | 至少 75%                   | 约 95%                              |
| 均值上下三个标准差        | 至少 88.888...%            | 约 99.73%                           |

## 中心极限定律

中心极限定理：**无论用于抽取样本的总体分布如何，带放回抽取的大型随机样本的总和或均值的概率分布大致是正态的。**

用于样本均值的中心极限定理：**如果从总体中带放回地抽取大型随机样本，那么不管总体分布情况如何，样本均值的概率分布大致是正态的，其频率分布曲线大致以总体均值为中心，标准差大致等于总体标准差除以样本量的平方根。**

### 样本均值的准确性

所有可能的样本均值的标准差表示样本均值的变化程度。**因此，它被视为样本均值作为总体均值的估计的准确度的一个度量。标准差越小，估计越准确。**

公式表明：

- 总体大小不影响样本均值的准确性。公式中的任何地方都没有出现总体大小。
- 总体标准差是一个常数；从总体中抽取的每个样本都是一样的。样本量可以变化。由于样本量出现在分母中，样本均值的可变性随着样本量的增加而降低，因此准确度增加。

平方根法则：一般来说，当你将样本量乘以一个因数时，用样本均值估计总体均值的准确度将会上升 该因数平方根 倍。

## 均值的置信区间的计算

![在这里插入图片描述](https://img-blog.csdnimg.cn/20191111220514986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80Mzk0ODM1Nw==,size_16,color_FFFFFF,t_70)

n为样本量

# 预测

## 相关系数

**相关系数测量两个变量之间线性关系的强度，通常缩写为相关性并用`r`表示。 在图形上，它测量散点图聚集在一条直线上的程度。**

**`r`是两个变量的乘积的均值，这两个变量都以标准单位来衡量。**

以下是一些关于`r`的数学事实，我们将通过模拟观察。

- 相关系数`r`是介于`-1`和`1`之间的数字。
- `r`度量了散点图围绕一条直线聚集的程度。
- 如果散点图是完美的向上倾斜的直线，`r = 1`，如果散点图是完美的向下倾斜的直线，`r = -1`。
- r的绝对值越大则两变量之间线性关系越强

## 相关不是因果

**相关只衡量关联，并不意味着因果。** 

> 尽管学区内的孩子的体重与数学能力之间的相关性可能是正的，但这并不意味着做数学会使孩子更重，或者说增加体重会提高孩子的数学能力。 年龄是一个使人混淆的变量：平均来说，较大的孩子比较小的孩子更重，数学能力更好。

## 相关性度量线性关联

相关性只测量一种关联 - 线性关联。 具有较强非线性关联的变量可能具有非常低的相关性。 

> 这里有一个变量的例子，它具有完美的二次关联`y = x ^ 2`，但是相关性等于 0。

## 相关性受到离群点影响

离群点可能对相关性有很大的影响。 下面是一个例子，其中通过增加一个离群点，`r`等于 1 的散点图变成`r`等于 0 的图。

```py
line = Table().with_columns(
        'x', make_array(1, 2, 3, 4),
        'y', make_array(1, 2, 3, 4)
    )
line.scatter('x', 'y', s=30, color='r')
```

![](https://box.kancloud.cn/bca750080587602a4bbb019cd27ca9da_384x358.png)

```py
correlation(line, 'x', 'y')
1.0
outlier = Table().with_columns(
        'x', make_array(1, 2, 3, 4, 5),
        'y', make_array(1, 2, 3, 4, 0)
    )
outlier.scatter('x', 'y', s=30, color='r')
```

![](https://box.kancloud.cn/0655f96bdcf4da27367a6ff4e5a157e5_376x358.png)

```py
correlation(outlier, 'x', 'y')
0.0
```

## 生态相关性应谨慎解读

基于汇总数据的相关性可能会产生误导。 作为一个例子，这里是 2014 年 SAT 批判性阅读和数学成绩的数据。50 个州和华盛顿特区各有一个点。`Participation Rate`列包含参加考试的高中学生的百分比。 接下来的三列显示了每个州的测试每个部分的平均得分，最后一列是测试总得分的平均值。

```py
sat2014 = Table.read_table('sat2014.csv').sort('State')
sat2014
```

| State                | Participation Rate | Critical Reading | Math | Writing | Combined |
| -------------------- | ------------------ | ---------------- | ---- | ------- | -------- |
| Alabama              | 6.7                | 547              | 538  | 532     | 1617     |
| Alaska               | 54.2               | 507              | 503  | 475     | 1485     |
| Arizona              | 36.4               | 522              | 525  | 500     | 1547     |
| Arkansas             | 4.2                | 573              | 571  | 554     | 1698     |
| California           | 60.3               | 498              | 510  | 496     | 1504     |
| Colorado             | 14.3               | 582              | 586  | 567     | 1735     |
| Connecticut          | 88.4               | 507              | 510  | 508     | 1525     |
| Delaware             | 100                | 456              | 459  | 444     | 1359     |
| District of Columbia | 100                | 440              | 438  | 431     | 1309     |
| Florida              | 72.2               | 491              | 485  | 472     | 1448     |

（省略了 41 行）

数学得分与批判性阅读得分的散点图紧密聚集在一条直线上; 相关性接近 0.985。

```py
sat2014.scatter('Critical Reading', 'Math')
```

![](https://box.kancloud.cn/e139f40d61e9d796c95aa2e824b04e52_391x358.png)

```py
correlation(sat2014, 'Critical Reading', 'Math')
0.98475584110674341
```

这是个非常高的相关性。但重要的是要注意，这并不能反映学生的数学和批判性阅读得分之间的关系强度。

数据由每个州的平均分数组成。但是各州不参加考试 - 而是学生。表中的数据通过将每个州的所有学生聚集为（这个州里面的两个变量的均值处的）单个点而创建。但并不是所有州的学生都会在这个位置，因为学生的表现各不相同。如果你为每个学生绘制一个点，而不是每个州一个点，那么在上图中的每个点周围都会有一圈云状的点。整体画面会更模糊。学生的数学和批判性阅读得分之间的相关性，将低于基于州均值计算的数值。

**基于聚合和均值的相关性被称为生态相关性，并且经常用于报告。正如我们刚刚所看到的，他们必须谨慎解读。**

## 回归直线

假设我们的估值函数（注意，是估计函数）:

![](https://pic2.zhimg.com/80/v2-7f0ed5769db393249c101032dec5b7a9_720w.webp)

![](https://pic1.zhimg.com/80/v2-37d56cd0b4384169833bdb05bf803adc_720w.webp)

## 最小二乘法

上述回归直线公式适合所有形状的散点图吗？

为了解决这些问题，我们需要一个“最优”的合理定义。回想一下，这条线的目的是预测或估计`y`的值，在给定`x`值的情况下。估计通常不是完美的。每个值都由于误差而偏离真正的值。“最优”直线的合理标准是，它在所有直线中总体误差尽可能最小。

对应于散点图上的每个点，预测的误差是计算为实际值减去预测值。 它是点与直线之间的垂直距离，如果点在线之下，则为负值。

如果我们用不同的线来创建我们的估计，误差将会不同。

![](https://box.kancloud.cn/47db447564cf570a241771e9371f7448_409x358.png)

![](https://box.kancloud.cn/d42ef1941610b4b83913c38f4021d117_409x358.png)

![](https://box.kancloud.cn/f2c1e7e8ce6f5e9beebc8f618e8477fa_409x358.png)

## 均方根误差（RMSE）

对应于散点图上的每个点，预测的误差是计算为实际值减去预测值。 它是点与直线之间的垂直距离，如果点在线之下，则为负值。

我们现在需要的是误差大小的一个总体衡量。 你会认识到创建它的方法 - 这正是我们开发标准差的方式。

如果你使用任意直线来计算你的估计值，那么你的一些误差可能是正的，而其他的则是负的。 为了避免误差大小在测量时抵消，**我们将采用误差平方的均值，称为均方误差（MSE）**

但均方误差单位很难解释，因此对**均方误差取平方根产生均方根误差（RMSE）**，与预测变量的单位相同，因此更容易理解。

## 使 RMSE 最小

到目前为止，我们的观察可以总结如下。

- 要根据`x`估算`y`，可以使用任何你想要的直线。
- 每个直线都有估计的均方根误差。
- “更好”的直线有更小的误差。

有没有“最好”的直线？ 也就是说，是否有一条线可以使所有行中的均方根误差最小？

回归线是所有直线之间的唯一直线，使估计的均方误差最小。这就是回归线有时被称为“最小二乘直线”的原因。

## 残差

假设数据科学家已经决定使用线性回归，基于预测变量估计响应变量的值。 为了了解这种估计方法的效果如何，数据科学家必须知道估计值距离实际值多远。 这些差异被称为残差。

散点图中的每个点都有残差，残差是回归线和点的垂直距离，即残差是`y`的观测值与`y`的预测值之间的差值

## 残差的性质

不管散点图的形状如何，残差的均值为0

无论散点图的形状如何，残差的标准差是观察值y的标准差的一个比例。

- 残差的标准差=根号下（1-r^2）* 观察值y的标准差

不管散点图的形状如何，拟合值的标准差是观察值`y`的标准差的一个比例

- |r|=拟合值标准差/观察值y的标准差

残差的标准差是如何衡量回归的好坏

- 请记住，残差的均值为 0。因此，残差的标准差越小，则残差越接近于 0。换句话说，如果残差的标准差小，那么回归中的总体误差就小。

## 残差图

残差图有助于我们直观评估线性回归分析的质量。 这种评估被称为诊断。函数`regression_diagnostic_plots`绘制原始散点图以及残差图，以便于比较。

![](https://box.kancloud.cn/ae3aa9a6de4961b7418de67d849cead6_389x371.png)

这个残差图表明，线性回归是合理的估计方法。 注意残差关于`y=0`的横线上下对称分布，相当于原始散点图关于回归线大致上下对称。

![](https://box.kancloud.cn/144ff3a83966cc620dbf839a05f0dc48_392x371.png)

在长度的较低一端，残差几乎都是正的；然后他们几乎都是负的；然后在较高一端，残差再次为正。 换句话说，回归估计值过高，然后过低，然后过高。 这意味着使用曲线而不是直线来估计年龄会更好。

![](https://box.kancloud.cn/34c02779afcec7d61b87fcd9c1fbea3b_389x371.png)

对于较低的加速度，误差的大小的变化比较高值更大。

如果残差图显示`y=0`的横线处的不均匀变化，则在预测变量的范围内，回归的估计不是同等准确的。

# 回归的推断

## 回归模型

大致线性的散点图中的一组随机性的假设称为回归模型。

回归模型认为，两个变量之间的底层关系是完全线性的

如果回归模型看起来合理，并且如果我们拥有大型样本，那么回归线就是真实直线的一个良好近似。 这使我们能够估计真实直线的斜率

### 估计真实斜率

当回归模型合理时，我们可以多次自举散点图，并绘制穿过每个自举图的回归线。 每条线都有一个斜率。 我们可以简单收集所有的斜率并绘制经验直方图，并打印由斜率的“中间 95%”组成的区间。

## 预测区间

回归的主要用途之一是对新个体进行预测，这个个体不是我们原始样本的一部分，但是与样本个体相似。在模型的语言中，我们想要估计新值`x`的`y`。

我们的估计是真实直线在`x`处的高度。当然，我们不知道真实直线。我们使用我们的样本点的回归线来代替。

我们可以像上一节那样，通过自举散点图来实现。 然后，我们为原始散点图的每个复制品拟合回归线，并根据新值x进行预测得到若干个预测值y。如果我们增加重采样过程的重复次数，我们可以生成预测的经验直方图。这将允许我们创建预测区间。最后，绘制所有预测值的经验直方图，并打印由预测值的“中间 95%”组成的区间。

![](https://box.kancloud.cn/856964a84921a6e7a904ef1280183c11_434x278.png)

![](https://box.kancloud.cn/e7a7d8d4747b0102ee74e5a039618f64_448x298.png)

## 注意事项

我们在本章中进行的所有预测和测试，都假设回归模型是成立的。 

如果散点图看起来不像那样，那么模型可能不适用于数据。 如果模型不成立，那么假设模型为真的计算是无效的。

因此，在开始基于模型进行预测，或者对模型参数进行假设检验之前，我们首先要确定回归模型是否适用于我们的数据。 一个简单的方法就是，按照我们在本节所做的操作，即绘制两个变量的散点图，看看它看起来是否大致线性，并均匀分布在一条线上。 我们还应该使用残差图，执行我们在前一节中开发的诊断。
